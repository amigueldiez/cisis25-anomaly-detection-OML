{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different approaches to anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python notebook proposes three different approaches for anomaly detection in network flows. In addition, their advantages and disadvantages are detailed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from river import anomaly\n",
    "from river import preprocessing\n",
    "from river import optim\n",
    "from river import sketch\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training flows for the online learning algorithm\n",
    "FLOWS_TRAIN_SCALER = 1000\n",
    "FLOWS_TRAIN_OML = 100000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset was created based on one of the datasets provided by Proactivanet. Subsequently, the following anomalies were introduced, with 2,500 flows of each type:\n",
    "\n",
    "- Benign IP connecting to an anomalous IP during working hours\n",
    "- Benign IP connecting to a benign IP at an anomalous time\n",
    "- Benign IP connecting to an anomalous domain during working hours\n",
    "- Benign IP connecting to an anomalous domain at an anomalous time\n",
    "- Anomalous IP connecting to an anomalous IP during working hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact Match Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH_DATASET2 = '../Dataset_real_Espiral/join_dataset_test1/'\n",
    "DATASET_NAME_DATASET2 = 'dataset_join_test1_anonymized.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV\n",
    "dataset = pd.read_csv(DATASET_PATH_DATASET2 + DATASET_NAME_DATASET2, sep=',')\n",
    "\n",
    "# Use the following features\n",
    "dataset = dataset[['FIRST_SWITCHED', 'IPV_SRC_ADDR', 'L_SRC_PORT', 'IPV_DST_ADDR', 'DIRECTION', 'L_DST_PORT', 'IN_BYTES', 'OUT_BYTES', 'Label']]\n",
    "\n",
    "# The FIRST_SWITCHED feature is converted into the hour of the day\n",
    "dataset['FIRST_SWITCHED'] = pd.to_datetime(dataset['FIRST_SWITCHED']).dt.hour\n",
    "\n",
    "# A PORT feature is created that is L_SRC_PORT if DIRECTION is 0 and L_DST_PORT if DIRECTION is 1\n",
    "dataset['PORT'] = np.where(dataset['DIRECTION'] == 0, dataset['L_SRC_PORT'], dataset['L_DST_PORT'])\n",
    "\n",
    "# Remove the columns L_SRC_PORT, L_DST_PORT, and DIRECTION\n",
    "dataset = dataset.drop(columns=['L_SRC_PORT', 'L_DST_PORT', 'DIRECTION'])\n",
    "\n",
    "# Preprocess IN_BYTES and OUT_BYTES by rounding to the nearest multiple of 100\n",
    "dataset['IN_BYTES'] = dataset['IN_BYTES'].apply(lambda x: round(x, -2))\n",
    "dataset['OUT_BYTES'] = dataset['OUT_BYTES'].apply(lambda x: round(x, -2))\n",
    "\n",
    "# Remove label\n",
    "dataset_sin_etiqueta = dataset.drop(columns=['Label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  100000\n"
     ]
    }
   ],
   "source": [
    "# Create the training set\n",
    "X_train = dataset_sin_etiqueta[dataset['Label'] == 0].iloc[:100000]\n",
    "print(\"Number of training samples: \", len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {}\n",
    "\n",
    "for row in X_train.iterrows():\n",
    "    key = tuple(row[1].values)\n",
    "    model[key] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El procedimiento anterior es igual al descrito anteriormente. Sin embargo, en este caso se ha entrenado con 200000 flujos benignos. En este caso, la fase de evaluaciÃ³n se ha realizado con 12500 flujos benignos y 12500 anÃ³malos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of anomalies in the test dataset:  12500\n",
      "Number of normal samples in the test dataset:  12500\n"
     ]
    }
   ],
   "source": [
    "# Create the test set\n",
    "X_test = dataset_sin_etiqueta[dataset['Label'] == 1].iloc[:12500]\n",
    "X_test = pd.concat([X_test, dataset_sin_etiqueta[dataset['Label'] == 0].iloc[100000:112500]], ignore_index=True)\n",
    "\n",
    "# Create the test set labels (1 for anomalies, 0 for normal flows)\n",
    "y_test = np.ones(12500)\n",
    "y_test = np.concatenate([y_test, np.zeros(12500)])\n",
    "\n",
    "print(\"Number of anomalies in the test dataset: \", y_test[y_test == 1].shape[0])\n",
    "print(\"Number of normal samples in the test dataset: \", y_test[y_test == 0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.741\n",
      "Recall:  1.0\n",
      "False Positive Rate:  0.518\n",
      "TP:  12500 TN:  6025 FP:  6475 FN:  0\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "for i, row in X_test.iterrows():\n",
    "\n",
    "    key = tuple(row.values)\n",
    "    is_anomaly = not key in model\n",
    "    label = y_test[i]\n",
    "    if is_anomaly and label == 1:\n",
    "        tp += 1\n",
    "    elif not is_anomaly and label == 0:\n",
    "        tn += 1\n",
    "    elif is_anomaly and label == 0:\n",
    "        fp += 1\n",
    "    elif not is_anomaly and label == 1:\n",
    "        fn += 1\n",
    "\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "false_positive = fp / (fp + tn)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"False Positive Rate: \", false_positive)\n",
    "print(\"TP: \", tp, \"TN: \", tn, \"FP: \", fp, \"FN: \", fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All anomalies are correctly detected (recall = 1), but the number of false positives is very high. This is due to the model's inability to generalize, as it relies on exact match searches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning - DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "dataset = dataset.sample(frac=1, random_state=111).reset_index(drop=True)\n",
    "\n",
    "# Create LabelEncoder objects\n",
    "le_src = LabelEncoder()\n",
    "le_dst = LabelEncoder()\n",
    "\n",
    "# Fit and transform the IPV_SRC_ADDR and IPV_DST_ADDR features\n",
    "dataset['IPV_SRC_ADDR'] = le_src.fit_transform(dataset['IPV_SRC_ADDR'])\n",
    "dataset['IPV_DST_ADDR'] = le_dst.fit_transform(dataset['IPV_DST_ADDR'])\n",
    "\n",
    "# Remove label column\n",
    "dataset_sin_etiqueta = dataset.drop(columns=['Label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the model was trained with 100,000 benign flows and 6,250 anomalous flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test sets\n",
    "X_train = dataset_sin_etiqueta[dataset['Label'] == 0].iloc[:93750]\n",
    "# y_train contains 0s\n",
    "y_train = np.zeros(93750)\n",
    "\n",
    "# Add 6,250 anomalous flows to the training set\n",
    "X_train = pd.concat([X_train, dataset_sin_etiqueta[dataset['Label'] == 1].iloc[:6250]])\n",
    "\n",
    "# Add 1s to y_train for the anomalous samples\n",
    "y_train = np.concatenate([y_train, np.ones(6250)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the training dataset:  100000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of samples in the training dataset: \", y_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeClassifier\n\u001b[1;32m      5\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcriterion\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgini\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m40\u001b[39m],\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_split\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m],\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m]\n\u001b[1;32m     10\u001b[0m }\n\u001b[0;32m---> 12\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39m\u001b[43mclf\u001b[49m, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Fit the grid search to the training data\u001b[39;00m\n\u001b[1;32m     15\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "# Create Grid Search for hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=123)\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30, 40],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters: \", best_params)\n",
    "# Create the Decision Tree Classifier with the best parameters\n",
    "clf = DecisionTreeClassifier(**best_params, random_state=987)\n",
    "# Fit the model\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation, 6,250 benign flows and 6,250 anomalous flows will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the anomalous flows from the dataset\n",
    "X_test = dataset_sin_etiqueta[dataset['Label'] == 1].iloc[6250:]\n",
    "y_test = np.ones(X_test.shape[0])\n",
    "\n",
    "# Add 6,250 benign flows to the test set\n",
    "X_test = pd.concat([X_test, dataset_sin_etiqueta[dataset['Label'] == 0].iloc[93750:100000]])\n",
    "y_test = np.concatenate([y_test, np.zeros(6250)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of samples in the test dataset: \", y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "tp = np.sum((y_pred == 1) & (y_test == 1))\n",
    "tn = np.sum((y_pred == 0) & (y_test == 0))\n",
    "fp = np.sum((y_pred == 1) & (y_test == 0))\n",
    "fn = np.sum((y_pred == 0) & (y_test == 1))\n",
    "\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "# If there are no true positives, recall is 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "false_positive = fp / (fp + tn)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"False Positive Rate: \", false_positive)\n",
    "print(\"TP: \", tp, \"TN: \", tn, \"FP: \", fp, \"FN: \", fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the results are better than those obtained with the exact match search. Most anomalies are successfully detected, and the number of false positives is lower. However, it is important to note that a labeled dataset is required to train the model. Furthermore, it is very difficult for the model to generalize across different networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Online Learning Using One-Class SVM (oSVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV\n",
    "dataset = pd.read_csv(DATASET_PATH_DATASET2 + DATASET_NAME_DATASET2, sep=',', index_col=False)\n",
    "\n",
    "# Features\n",
    "FEATURES = ['IPV_SRC_ADDR', 'IPV_DST_ADDR', 'L_DST_PORT', 'L_SRC_PORT', 'DIRECTION', 'FIRST_SWITCHED', 'LAST_SWITCHED', 'PROTOCOL', 'END_TYPE', 'IN_BYTES', 'OUT_BYTES', 'Label']\n",
    "\n",
    "# Preprocess flows\n",
    "dataset = dataset[FEATURES]\n",
    "dataset = dataset.iloc[:, :].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Significative port\n",
    "print(\"â„¹ï¸ | MODE SIGNIFICANT PORT ACTIVATED\")\n",
    "for i in range(len(dataset)):\n",
    "    if dataset[i][FEATURES.index('DIRECTION')] == 1:\n",
    "        dataset[i][FEATURES.index('DIRECTION')] = dataset[i][FEATURES.index('L_DST_PORT')]\n",
    "    else:\n",
    "        dataset[i][FEATURES.index('DIRECTION')] = dataset[i][FEATURES.index('L_SRC_PORT')]\n",
    "print('âœ… | Significant port added')\n",
    "    \n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i][FEATURES.index('FIRST_SWITCHED')] = int(dataset[i][FEATURES.index('FIRST_SWITCHED')].split(' ')[1].split(':')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform numpy array to pandas dataframe\n",
    "dataset = pd.DataFrame(dataset, columns=FEATURES)\n",
    "\n",
    "# Significant port\n",
    "dataset = dataset.drop(columns=['L_SRC_PORT', 'L_DST_PORT'])\n",
    "# Change DIRECTION column name to PORT\n",
    "dataset = dataset.rename(columns={'DIRECTION': 'PORT'})\n",
    "\n",
    "\n",
    "\n",
    "# Create the two datasets for trainning\n",
    "datasets_training = dataset.loc[dataset['Label'] == 0].iloc[:FLOWS_TRAIN_SCALER + FLOWS_TRAIN_OML]\n",
    "dataset_train_scaler = datasets_training.loc[dataset['Label'] == 0].iloc[:FLOWS_TRAIN_SCALER]\n",
    "dataset_train_oml = datasets_training.loc[dataset['Label'] == 0].iloc[FLOWS_TRAIN_SCALER:FLOWS_TRAIN_SCALER + FLOWS_TRAIN_OML + 1]\n",
    "\n",
    "\n",
    "# Drop train_scaler and train_oml from the dataset\n",
    "dataset = dataset.drop(dataset_train_scaler.index)\n",
    "dataset = dataset.drop(dataset_train_oml.index)\n",
    "\n",
    "\n",
    "# Number of anomalies and normal samples\n",
    "anomalies_samples= dataset.loc[dataset['Label'] == 1].shape[0]\n",
    "benign_samples = dataset.loc[dataset['Label'] == 0].shape[0]\n",
    "\n",
    "# Drop benign samples to equilibrate the dataset\n",
    "if benign_samples > anomalies_samples:\n",
    "    dataset = dataset.drop(dataset.loc[dataset['Label'] == 0].index[:benign_samples - anomalies_samples])\n",
    "else:\n",
    "    dataset = dataset.drop(dataset.loc[dataset['Label'] == 1].index[:anomalies_samples- benign_samples])\n",
    "\n",
    "# Create the test dataset\n",
    "dataset_test_labeled = dataset\n",
    "\n",
    "# Shuffle the datasets\n",
    "dataset_train_scaler = dataset_train_scaler.sample(frac=1,random_state=111).reset_index(drop=True)\n",
    "dataset_train_oml = dataset_train_oml.sample(frac=1,random_state=111).reset_index(drop=True)\n",
    "dataset_test_labeled = dataset_test_labeled.sample(frac=1,random_state=111).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.OrdinalEncoder()\n",
    "\n",
    "for i in range(len(dataset_train_scaler)):\n",
    "    # Obtain first two elements (IP addresses) pandas datraframe\n",
    "    first_two_elements = {j: str(dataset_train_scaler.iloc[i, j]) for j in range(2)}\n",
    "    # Encode the IP addresses\n",
    "    encoder.learn_one(first_two_elements)\n",
    "    first_two_elements = encoder.transform_one(first_two_elements)\n",
    "    dataset_train_scaler.iloc[i, 0] = first_two_elements[0]\n",
    "    dataset_train_scaler.iloc[i, 1] = first_two_elements[1]\n",
    "\n",
    "for i in range(len(dataset_train_oml)):\n",
    "    # Obtain first two elements (IP addresses)\n",
    "    first_two_elements = {j: str(dataset_train_oml.iloc[i, j]) for j in range(2)}\n",
    "    # Encode the IP addresses\n",
    "    encoder.learn_one(first_two_elements)\n",
    "    first_two_elements = encoder.transform_one(first_two_elements)\n",
    "    dataset_train_oml.iloc[i, 0] = first_two_elements[0]\n",
    "    dataset_train_oml.iloc[i, 1] = first_two_elements[1]\n",
    "\n",
    "for i in range(len(dataset_test_labeled)):\n",
    "    # Obtain first two elements (IP addresses)\n",
    "    first_two_elements = {j: str(dataset_test_labeled.iloc[i, j]) for j in range(2)}\n",
    "    # Encode the IP addresses\n",
    "    encoder.learn_one(first_two_elements)\n",
    "    first_two_elements = encoder.transform_one(first_two_elements)\n",
    "    dataset_test_labeled.iloc[i, 0] = first_two_elements[0]\n",
    "    dataset_test_labeled.iloc[i, 1] = first_two_elements[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=====================================================================================================')\n",
    "print('Number of anomalies in the train scaler dataset:\\t', dataset_train_scaler.loc[dataset_train_scaler['Label'] == 1].shape[0])\n",
    "print('Number of normal samples in the train scaler dataset:\\t', dataset_train_scaler.loc[dataset_train_scaler['Label'] == 0].shape[0])\n",
    "\n",
    "print('Number of normal samples in the trainOML dataset:\\t', dataset_train_oml.loc[dataset_train_oml['Label'] == 0].shape[0])\n",
    "print('Number of anomaly samples in the trainOML dataset:\\t', dataset_train_oml.loc[dataset_train_oml['Label'] == 1].shape[0])\n",
    "\n",
    "print('Number of anomalies in the test dataset:\\t', dataset_test_labeled.loc[dataset_test_labeled['Label'] == 1].shape[0])\n",
    "print('Number of normal samples in the test dataset:\\t', dataset_test_labeled.loc[dataset_test_labeled['Label'] == 0].shape[0])\n",
    "print('=====================================================================================================')\n",
    "\n",
    "# Obtain the datasets without the label\n",
    "dataset_train_no_labels = np.delete(dataset_train_oml, -1, axis=1)\n",
    "dataset_test_no_labels = np.delete(dataset_test_labeled, -1, axis=1)\n",
    "dataset_train_scaler = dataset_train_scaler.drop(columns=['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MaxAbsScaler()\n",
    "\n",
    "scaler_dataset_train = []\n",
    "scaler_dataset_test = []\n",
    "\n",
    "# Convert the dataset to pandas DataFrame format\n",
    "dataset_train_scaler = pd.DataFrame(dataset_train_scaler)\n",
    "dataset_train_no_labels = pd.DataFrame(dataset_train_no_labels)\n",
    "dataset_test_no_labels = pd.DataFrame(dataset_test_no_labels)\n",
    "\n",
    "# Train the scaler model using the training dataset\n",
    "for _, row in dataset_train_scaler.iterrows():\n",
    "    # Convert row to dict using keys 0, 1, 2, 3, ...\n",
    "    row = {i: value for i, value in enumerate(row)}\n",
    "    scaler.learn_one(row)\n",
    "\n",
    "# Scale the training dataset using the trained scaler\n",
    "for _, row in dataset_train_no_labels.iterrows():\n",
    "    row = row.to_dict()\n",
    "    row = scaler.transform_one(row)\n",
    "    scaler_dataset_train.append(list(row.values()))\n",
    "\n",
    "# Scale the test dataset using the previously trained scaler\n",
    "for _, row in dataset_test_no_labels.iterrows():\n",
    "    row = row.to_dict()\n",
    "    row = scaler.transform_one(row)\n",
    "    scaler_dataset_test.append(list(row.values()))\n",
    "\n",
    "print(\"âœ… | Dataset Scaled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = anomaly.QuantileFilter(\n",
    "        anomaly.OneClassSVM(nu=0.05,intercept_lr=optim.schedulers.InverseScaling(learning_rate=0.25)),\n",
    "        q = 0.99\n",
    "    )\n",
    "\n",
    "probability_preprocessing = preprocessing.MinMaxScaler()\n",
    "probability = sketch.Histogram()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traning phase of OML\n",
    "print(\"â³ | Training OML\")\n",
    "for row in scaler_dataset_train:\n",
    "    # Change dict to numpy array\n",
    "    row_dict = {f'feature_{i}': value for i, value in enumerate(row)}\n",
    "\n",
    "    model.learn_one(row_dict)\n",
    "\n",
    "    score = model.score_one(row_dict)\n",
    "    probability_preprocessing.learn_one({0: score})\n",
    "    probability.update(probability_preprocessing.transform_one({0: score})[0])\n",
    "\n",
    "print(\"âœ… | Training phase completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = 0\n",
    "fn = 0\n",
    "tp = 0\n",
    "tn = 0\n",
    "accuracies = []\n",
    "recalls = []\n",
    "false_positives = []\n",
    "anomalies = []\n",
    "\n",
    "\n",
    "\n",
    "for i, row in enumerate(scaler_dataset_test):\n",
    "    # To dict\n",
    "    if isinstance(row, list):\n",
    "        row = {f'feature_{j}': value for j, value in enumerate(row)}\n",
    "\n",
    "    score = model.score_one(row)\n",
    "    anomalo = model.classify(score)\n",
    "    anomalies.append(anomalo)\n",
    "\n",
    "    # Update probability\n",
    "    probability_preprocessing.learn_one({0: score})\n",
    "    probability.update(probability_preprocessing.transform_one({0: score})[0])\n",
    "    rank = probability.cdf(probability_preprocessing.transform_one({0: score})[0])\n",
    "\n",
    "\n",
    "    if not anomalo:\n",
    "        model.learn_one(row)\n",
    "\n",
    "    label = dataset_test_labeled.iloc[i, -1]\n",
    "\n",
    "    if anomalo and label == 1:\n",
    "        tp += 1\n",
    "    elif not anomalo and label == 0:\n",
    "        tn += 1\n",
    "    elif anomalo and label == 0:\n",
    "        fp += 1\n",
    "    elif not anomalo and label == 1:\n",
    "        fn += 1\n",
    "        \n",
    "    accuracies.append((tp + tn) / (tp + tn + fp + fn))\n",
    "    recalls.append(tp / (tp + fn) if (tp + fn) > 0 else 0)\n",
    "    false_positives.append(fp / (fp + tn) if (fp + tn) > 0 else 0)\n",
    "\n",
    "\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "false_positive = fp / (fp + tn)\n",
    "print(\"âœ… | Testing phase completed\")\n",
    "print(\"ðŸŸ¢ | Accuracy: \", accuracy)\n",
    "print(\"ðŸŸ¢ | Recall: \", recall)\n",
    "print(\"ðŸŸ¢ | False Positive Rate: \", false_positive)\n",
    "print(\"TP: \", tp, \"TN: \", tn, \"FP: \", fp, \"FN: \", fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best results were achieved by this model, which is capable of detecting anomalies without relying on a labeled datasetâ€”in other words, by analyzing the underlying patterns in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
